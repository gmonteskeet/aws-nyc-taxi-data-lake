# Week #4 - Procesamiento, modelado y consultas en el Data Lake
Esta es la **parte final del** proyecto de la TLC.

Los objetivos de este proyecto son:
* Implementar la capa *structured* del Data Lake
* Hacer consultables los datos via AWS Athena 

# DescripciÃ³n del proyecto
En este proyecto se finalizarÃ¡ la implementaciÃ³n de un data lake estructurado por zonas.

Hemos estado siguiendo la estructura de carpetas propuesta por [Zaloni](https://www.oreilly.com/library/view/architecting-data-lakes/9781492033004/).  

Sin embargo, es importante seÃ±alar que no es la Ãºnica que existe. Otra famosa es la [Medallion Architecture](https://www.databricks.com/glossary/medallion-architecture) propuesta por Databricks.

Dicho eso, vamos con la de Zaloni:
* **Landing:** capa de trÃ¡nsito donde los datos se ingestan a la nube para hacer chequeos de calidad y determinar si se promueven a `raw` o `quarantine`.  
* **Raw:** copia fidedigna de los datos tal cual como se encuentran en la fuente. AquÃ­ no se hacen transformaciones (solo se aÃ±ade una columna de ingesta para trazabilidad).  
* **Structured:** primera capa *consultable* del data lake. AquÃ­ se garantiza la integridad del dataset, el esquema es consistente, las columnas estÃ¡n tipadas correctamente y las particiones estÃ¡n organizadas para ser leÃ­das por motores externos como Athena.  
* **Quarantine:** donde se aÃ­slan los datos que no cumplen los chequeos de calidad. **Se debe** implementar una estrategia de intervenciÃ³n (diagnÃ³stico, autoreparaciÃ³n, reprocesamiento, etc.).  

La siguiente es la arquitectura sugerida para este proyecto:
![Texto alternativo](/img/final_architecture.png)

## Outcome final 
El resultado final esperado es automatizar la ingesta para los datasets de Yellow Trips y Green Trips.  

**Sin embargo, para facilitar el aprendizaje, se recomienda comenzar por Yellow Trips** y, si tienes tiempo, avanzar a Green Trips.

La soluciÃ³n final debe:
- Ingestar los datos del dataset de Yellow Trips para los aÃ±os 2024 y 2025 (mÃ­nimo requerido).
- Mapear los datos en catÃ¡logo de Glue.
- Hacer los datos consultables a travÃ©s de AWS Athena.
- Orquestar toda la ingesta utilizando Apache Airflow.

## Entregables
- CÃ³digo final de Airflow
- ImÃ¡genes donde se muestren que los datos ingestados estÃ¡n estructurados en S3 como se sugiere en la arquitectura.
- Evidencia (imagen)  de los datasets mapeados en AWS Glue Catalog.
- Evidencia (imagen)  de los datasets consultables a travÃ©s de AWS Athena.
- Consultas SQL utilizadas.

---

# Instrucciones

## 1. Implementar la capa *structured*  
Debes crear un _job_ en AWS Glue que tome los archivos desde `raw` y los escriba en `structured`.

Este job debe:
* Estandarizar tipos de datos: fechas (`date`), timestamps (`timestamp`), numÃ©ricos (`double`, `bigint`), etc.
* Enriquecer los datos aÃ±adiendo nuevas columnas. Por ejemplo, los timestamps puede partirse en dos columnas, una con fecha y otra con el tiempo.
* La escritura en esta zona se debe hacer particionando los datos. Esto permite reducir costos y optimizar el rendimiento de las consultas. Se recomienda seguir la convenciÃ³n mÃ¡s comÃºn: `source/ds=YYYY-MM-DD`.

> ðŸ“Œ Para entender mÃ¡s acerca de particionamiento se recomienda revisar los siguientes recursos:
> * [What is partitioning?](https://docs.aws.amazon.com/athena/latest/ug/ctas-partitioning-and-bucketing-what-is-partitioning.html)
> * [Top 10 Performance Tuning Tips for Amazon Athena](https://aws.amazon.com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/)

## 2. Registrar las tablas en Glue Catalog  

Una vez tengas los datos en `structured`, debes registrarlos en Glue Catalog para que sean consultables desde Athena.

Para ello se recomienda usar un Glue Crawler que apunte a `.../structured/yellow-trips/`. El crawler detectarÃ¡ automÃ¡tica mente las particiones y crearÃ¡ una tabla en el catÃ¡logo de glue.

> ðŸ“Œ [AWS Glue Tutorial for Beginners| Learn everything about Glue in 30 mins| Glue Data Catalog| Glue ETL](https://www.youtube.com/watch?v=weWeaM5-EHc)

## 3. Consultar los datos en Athena  
Con la tabla registrada, abre Athena y valida que se puedan consultar.

Ejemplos de consultas:

```
sql
SELECT 
    COUNT(*) 
FROM yellow_trips;

SELECT 
    ds, 
    COUNT(*) 
FROM yellow_trips 
GROUP BY ds;

SELECT 
    year, month, COUNT(*) 
FROM yellow_trips 
GROUP BY year, month;

SELECT MAX(trip_distance) AS max_dist, year
FROM yellow_trips
GROUP BY year;

SELECT payment_type, COUNT(*)
FROM yellow_trips
GROUP BY payment_type;
```

## 4. Integrar todo dentro de Airflow
Actualiza tu DAG para incluir la capa structured.

El DAG debe lucir de la siguiente manera:

```
start
  â†’ lambda_ingest
  â†’ lambda_landing_to_raw
  â†’ glue_raw_to_structured
  â†’ glue_raw_to_structured
  â†’ end
```

Ten en cuenta:
* Se debe hacer un backfill con todo el pipeline construido para 2024 y 2025.

## Capas opcionales
### 1. Implementa procesos adicionales para la capa de quarantine
Si seguiste todos los pasos de manera adecuada, el resultado final de este proyecto serÃ¡:

* Tener todos los archivos disponibles de 2025 ingestados en `structured` y consultables desde Athena.  
* Tener archivos de aÃ±os anteriores (por ejemplo, 2024) enviados correctamente a `quarantine` cuando no cumplen las validaciones.

Ahora bien, enviar datos a `quarantine` **no es el final del proceso**.  
Un buen data lake debe dar visibilidad sobre *quÃ© saliÃ³ mal* y ofrecer mecanismos para recuperar esos datos.

Cuando enviamos archivos a la zona de cuarentena, el pipeline debe permitir conocer:

* **QuÃ© regla fallÃ³** (inconsistencias de esquema, valores nulos no permitidos, formatos invÃ¡lidos, columnas faltantes, etc.).  
* **CuÃ¡ntos registros fallaron** y en quÃ© categorÃ­a.  

Por eso, una capa opcionalâ€”pero altamente recomendadaâ€” es implementar una **capa adicional de observabilidad y remediaciÃ³n**, que:

1. **Registre los errores** asociados a cada archivo o registro (la manera mÃ¡s simple es a travÃ©s de la generaciÃ³n de logs).  
2. **Implemente estrategias de correcciÃ³n automÃ¡tica** siguiendo reglas explÃ­citas de negocio.

Un ejemplo de estrategia de remediaciÃ³n es:

* Crear procesos que apliquen **correcciones automÃ¡ticas**: estandarizaciÃ³n de formatos, conversiÃ³n de tipos, imputaciÃ³n de valores por defecto, normalizaciÃ³n de categorÃ­as, etc.
* Una vez los datos corregidos cumplan las validaciones, permitir que **se promuevan automÃ¡ticamente** a las capas `raw` y `structured`, integrÃ¡ndose de nuevo al flujo principal del pipeline.

Esto convierte la zona de quarantine no en un â€œbasureroâ€, sino en una **zona de recuperaciÃ³n controlada**, donde los datos pueden volver a ser Ãºtiles sin intervenciÃ³n manual permanente.

### 2. Convierte tu data lake en un lakehouse
Si quieres llevar este proyecto a un nivel mucho mÃ¡s profesional, puedes transformar tu data lake en un **lakehouse** implementando un *open table format* como **Apache Iceberg**.

Â¿Por quÃ© Iceberg?  
Porque soluciona muchas de las limitaciones histÃ³ricas de los data lakes tradicionales y te permite trabajar con tus datos como si fueran tablas transaccionales, con beneficios como:

* **Esquemas evolutivos** (aÃ±adir o modificar columnas sin recrear tablas).  
* **Time travel** para consultar versiones anteriores de los datos.  
* **Operaciones ACID** en S3 (insert, update, delete, merge).  
* **Alto rendimiento en consultas** gracias a metadata optimizada.  

En pocas palabras:  
Iceberg convierte tu carpeta `structured` en una **tabla transaccional robusta**, manteniendo S3 como almacenamiento pero dÃ¡ndote garantÃ­as de un warehouse moderno.

>ðŸ“Œ [Set Up and Use Apache Iceberg Tables on Your Data Lake - AWS Virtual Workshop](https://www.youtube.com/watch?v=SZDYmWIStUo)



---

# Entrega
- El plazo de entrega de este proyecto es el **Martes 16 de Diciembre antes de la medianoche.**
- La entrega se hace a travÃ©s de Github. Para eso, haz un fork a este repositorio y aÃ±Ã¡deme como colaborador a tu repositorio para poder hacer las veces de revisor.
- Recuerda manejar las ramas de git para hacer el desarrollo.
- Realiza PRs (Pull requests) para las diferentes funcionalidades que implementes. 
- En cada PR asÃ­gname como revisor en Github. No hagas merge hasta que yo apruebe el PR.

> ðŸ“Œ **Nota:** Si nunca haz creado un PR, [acÃ¡](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/creating-a-pull-request) puedes encontrar una breve explicaciÃ³n de cÃ³mo hacerlo.

---
# Recursos tÃ©cnicos
Spark:
* [Learning Spark - data-engineering-zoomcamp](https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/05-batch)
* [Apache Spark - The Ultimate Guide [From ZERO To PRO]](https://www.youtube.com/watch?v=FNJze2Ea780)
* [AWS Glue Tutorial for Beginners [NEW 2024 - FULL COURSE]](https://youtu.be/ZvJSaioPYyo?si=O_uerpwPPaaGtHfj)

Apache Iceberg
- [Why Open Table Format Architecture is Essential for Modern Data Systems](https://www.phdata.io/blog/why-open-table-format-architecture-is-essential-for-modern-data-systems/)
- [Using Apache Iceberg on AWS](https://docs.aws.amazon.com/prescriptive-guidance/latest/apache-iceberg-on-aws/introduction.html)
- [Apache Iceberg on AWS with S3 and Athena (Demo)](https://www.youtube.com/watch?v=iGvj1gjbwl0)
- [Apache Iceberg vs Delta Lake â€“ Which Open Table Format Should You Choose in 2025?](https://www.youtube.com/watch?v=ob2i3PpaZyg)
